{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1942361,"sourceType":"datasetVersion","datasetId":877017}],"dockerImageVersionId":30157,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sagniksanyal/tweet-s-text-classicifaction?scriptVersionId=87268101\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T05:59:41.289651Z","iopub.execute_input":"2022-02-08T05:59:41.290464Z","iopub.status.idle":"2022-02-08T05:59:41.301613Z","shell.execute_reply.started":"2022-02-08T05:59:41.290405Z","shell.execute_reply":"2022-02-08T05:59:41.30051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\nimport os\nimport itertools\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom PIL import Image\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom nltk.util import ngrams\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport re\nfrom collections import Counter\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport requests\nimport json\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:59:41.312737Z","iopub.execute_input":"2022-02-08T05:59:41.312983Z","iopub.status.idle":"2022-02-08T05:59:41.331949Z","shell.execute_reply.started":"2022-02-08T05:59:41.312954Z","shell.execute_reply":"2022-02-08T05:59:41.330789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/the-social-dilemma-tweets/TheSocialDilemma.csv'\ndata = pd.read_csv(file_path)\ndata = data[['text', 'Sentiment']]\ndata.head()\n\ndef clean_text(text):\n  text = text.lower()\n  text = re.sub('\\[.*?\\]', '', text)\n  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n  text = re.sub('\\n', '', text)\n  text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n  return text\ndata['text'] = data['text'].apply(lambda x: clean_text(x))\n\nX = data['text']\ny = data['Sentiment'].map({'Negative':0, 'Neutral':1, 'Positive':2})\n\ntrain_size = int(len(data)*0.8)\nX_train, y_train = X[:train_size], y[:train_size]\nX_test, y_test = X[train_size:], y[train_size:]\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)\n\nprint ('Length of text: {} characters'.format(len(X_train)))\n\nprint(\"Max tweet length:\", X.map(len).max())\nprint(\"Min tweet length:\", X.map(len).min())\nprint(\"Average tweet length:\", X.map(len).mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:59:41.334698Z","iopub.execute_input":"2022-02-08T05:59:41.335189Z","iopub.status.idle":"2022-02-08T05:59:41.823702Z","shell.execute_reply.started":"2022-02-08T05:59:41.335141Z","shell.execute_reply":"2022-02-08T05:59:41.822624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 8000\nembedding_dim = 32\nmax_length = 90\ntokenizer = Tokenizer(vocab_size)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(X_train)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding='pre', truncating='pre')\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding='pre', truncating='pre')\n\nprint(\"Shape of train_padded:\", train_padded.shape)\nprint(\"Shape of test_padded:\", test_padded.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:59:41.825217Z","iopub.execute_input":"2022-02-08T05:59:41.827077Z","iopub.status.idle":"2022-02-08T05:59:42.909301Z","shell.execute_reply.started":"2022-02-08T05:59:41.82704Z","shell.execute_reply":"2022-02-08T05:59:42.908237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n                    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n                    tf.keras.layers.LSTM(100),\n                    tf.keras.layers.Dense(max_length/2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n                    tf.keras.layers.Dropout(0.4),\n                    tf.keras.layers.Dense(3, activation='softmax')\n])\nmodel.summary()\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    0.01,\n    decay_steps=10000,\n    decay_rate=0.95,\n    staircase=True\n)\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = lr_schedule),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_padded, y_train, epochs=30, validation_data=(test_padded, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T05:59:42.912318Z","iopub.execute_input":"2022-02-08T05:59:42.912632Z","iopub.status.idle":"2022-02-08T06:02:21.140937Z","shell.execute_reply.started":"2022-02-08T05:59:42.912586Z","shell.execute_reply":"2022-02-08T06:02:21.139905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_encode(x):\n  #x = clean_text(x)\n  x = tokenizer.texts_to_sequences(x)\n  x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=max_length, padding='pre', truncating='pre')\n  return x\n\ntest_comment = ['This movie depicted the current society issues so well, I loved it so much']\n\nseq = tokenizer.texts_to_sequences(test_comment)\npadded = pad_sequences(seq, maxlen=max_length, padding='pre', truncating='pre')\nprint(padded.shape)\ny_pred = model.predict(padded).round()\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:21.142623Z","iopub.execute_input":"2022-02-08T06:02:21.143421Z","iopub.status.idle":"2022-02-08T06:02:21.589622Z","shell.execute_reply.started":"2022-02-08T06:02:21.143344Z","shell.execute_reply":"2022-02-08T06:02:21.588545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path=(\"../input/the-social-dilemma-tweets/TheSocialDilemma.csv\")\nsocial = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:21.592887Z","iopub.execute_input":"2022-02-08T06:02:21.59313Z","iopub.status.idle":"2022-02-08T06:02:21.75074Z","shell.execute_reply.started":"2022-02-08T06:02:21.593098Z","shell.execute_reply":"2022-02-08T06:02:21.749771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"social[social['user_name']=='OurPact'][['text','Sentiment']]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:21.752519Z","iopub.execute_input":"2022-02-08T06:02:21.752902Z","iopub.status.idle":"2022-02-08T06:02:21.774493Z","shell.execute_reply.started":"2022-02-08T06:02:21.752841Z","shell.execute_reply":"2022-02-08T06:02:21.773159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"social[social['user_name']=='OurPact']['Sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:21.776265Z","iopub.execute_input":"2022-02-08T06:02:21.776892Z","iopub.status.idle":"2022-02-08T06:02:21.793236Z","shell.execute_reply.started":"2022-02-08T06:02:21.776845Z","shell.execute_reply":"2022-02-08T06:02:21.791841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"social['user_created'] = pd.to_datetime(social['user_created'])\nsocial['year_created'] = social['user_created'].dt.year\ndata = social.drop_duplicates(subset='user_name', keep=\"first\")\ndata = data[data['year_created']>1970]\ndata = data['year_created'].value_counts().reset_index()\ndata.columns = ['year', 'number']\n\nfig = sns.barplot( \n    x=data[\"year\"], \n    y=data[\"number\"], \n    orientation='vertical'\n    #title='', \n).set_title('User created year by year')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:21.795016Z","iopub.execute_input":"2022-02-08T06:02:21.796457Z","iopub.status.idle":"2022-02-08T06:02:22.898982Z","shell.execute_reply.started":"2022-02-08T06:02:21.796406Z","shell.execute_reply":"2022-02-08T06:02:22.897742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\ndef pie_count(data, field, percent_limit, title):\n    \n    data[field] = data[field].fillna('NA')\n    data = data[field].value_counts().to_frame()\n\n    total = data[field].sum()\n    data['percentage'] = 100 * data[field]/total    \n\n    percent_limit = percent_limit\n    otherdata = data[data['percentage'] < percent_limit] \n    others = otherdata['percentage'].sum()  \n    maindata = data[data['percentage'] >= percent_limit]\n\n    data = maindata\n    other_label = \"Others(<\" + str(percent_limit) + \"% each)\"\n    data.loc[other_label] = pd.Series({field:otherdata[field].sum()}) \n    \n    labels = data.index.tolist()   \n    datavals = data[field].tolist()\n    \n    trace=go.Pie(labels=labels,values=datavals)\n    \n    layout = go.Layout(\n        title = title,\n        height=600,\n        width=600\n        )\n    \n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \npie_count(social, 'user_location', 0.5, 'Number of tweets per location')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:22.905582Z","iopub.execute_input":"2022-02-08T06:02:22.906006Z","iopub.status.idle":"2022-02-08T06:02:22.998465Z","shell.execute_reply.started":"2022-02-08T06:02:22.905958Z","shell.execute_reply":"2022-02-08T06:02:22.997572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tweet per day**","metadata":{}},{"cell_type":"code","source":"social['tweet_date']=pd.to_datetime(social['date']).dt.date\ntweet_date=social['tweet_date'].value_counts().to_frame().reset_index().rename(columns={'index':'date','tweet_date':'count'})\ntweet_date['date']=pd.to_datetime(tweet_date['date'])\ntweet_date=tweet_date.sort_values('date',ascending=False)\nfig=go.Figure(go.Scatter(x=tweet_date['date'],\n                                y=tweet_date['count'],\n                               mode='markers+lines',\n                               name=\"Submissions\",\n                               marker_color='dodgerblue'))\n\nfig.update_layout(\n    title_text='Tweets per Day : ({} - {})'.format(social['tweet_date'].sort_values()[0].strftime(\"%d/%m/%Y\"),\n                                                       social['tweet_date'].sort_values().iloc[-1].strftime(\"%d/%m/%Y\")),template=\"plotly_dark\",\n    title_x=0.5)\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:23.005031Z","iopub.execute_input":"2022-02-08T06:02:23.005379Z","iopub.status.idle":"2022-02-08T06:02:23.154614Z","shell.execute_reply.started":"2022-02-08T06:02:23.005335Z","shell.execute_reply":"2022-02-08T06:02:23.153563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Refining the text **","metadata":{}},{"cell_type":"code","source":"def remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text\ndef remove_mention(text):\n    line=re.sub(r'@\\w+','',text)\n    return line\ndef remove_hash(text):\n    line=re.sub(r'#\\w+','',text)\n    return line\n\ndef remove_newline(string):\n    text=re.sub('\\n','',string)\n    return text\ndef remove_url(string): \n    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n    return text\ndef remove_number(text):\n    line=re.sub(r'[0-9]+','',text)\n    return line\ndef remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n    return line\ndef text_strip(string):\n    line=re.sub('\\s{2,}', ' ', string.strip())\n    return line\ndef remove_thi_amp_ha_words(string):\n    line=re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b',' ',string)\n    return line","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:23.155966Z","iopub.execute_input":"2022-02-08T06:02:23.156389Z","iopub.status.idle":"2022-02-08T06:02:23.167906Z","shell.execute_reply.started":"2022-02-08T06:02:23.156336Z","shell.execute_reply":"2022-02-08T06:02:23.166814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**N-GRAM**","metadata":{}},{"cell_type":"code","source":"social['refine_text']=social['text'].str.lower()\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_tag(str(x)))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_mention(str(x)))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_hash(str(x)))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_newline(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_url(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_number(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_punct(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:remove_thi_amp_ha_words(x))\nsocial['refine_text']=social['refine_text'].apply(lambda x:text_strip(x))\n\nsocial['text_length']=social['refine_text'].str.split().map(lambda x: len(x))\ndef ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\nunigram_df=ngram_df(social['refine_text'],(1,1),20)\nbigram_df=ngram_df(social['refine_text'],(2,2),20)\ntrigram_df=ngram_df(social['refine_text'],(3,3),20)\nfig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top N Grams',xaxis_title=\" \",yaxis_title=\" \",\n                  showlegend=False,title_x=0.5,height=1200,template=\"plotly_dark\")\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T06:02:23.169723Z","iopub.execute_input":"2022-02-08T06:02:23.170399Z","iopub.status.idle":"2022-02-08T06:02:27.538167Z","shell.execute_reply.started":"2022-02-08T06:02:23.170325Z","shell.execute_reply":"2022-02-08T06:02:27.537179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* social, watch, and media are the most used unigrams\n* social media, social dilemma, just watched are the most used bigrams in the social dilemma tweets\n* social dilemma neflix, wach social dilemma, keepin kids safe, kids safe online are the most used trigrams!","metadata":{}}]}